Batch sweep helper and analysis notes

This note explains how to run batch-size experiments against the SurrealDB importer/streaming code and how to interpret the produced metrics.

Files
- `scripts/batch_sweep.sh`: a convenience script that runs the `hyperzoekt` binary with several `batch_capacity` and `batch_timeout_ms` configurations and writes JSON metrics files to `.data/` named `db_metrics_<cap>_<timeout>_<mode>.json`.

How it works
- The script generates temporary TOML config files in `.tmp_experiments/` and sets `SURREAL_METRICS_FILE` to capture metrics per run.
- It supports two modes: `streaming` (send payloads via the DB batching thread) and `initial_batch` (send initial batch using the DB thread's initial-batch path).
- The metrics file contains: batches_sent, entities_sent, avg_batch_ms, min_batch_ms, max_batch_ms, batch_failures, attempt_counts, etc.

Run it
```bash
./scripts/batch_sweep.sh
```

Interpretation
- entities_sent / batches_sent gives average batch size.
- avg_batch_ms / min/max give latency characteristics; use these to find the knee where increasing batch size stops improving throughput.
- batch_failures and attempt_counts indicate DB-side problems or timeouts; prefer configurations with low failures and low avg_batch_ms.

Next steps
- Add a small summarizer that aggregates all `.data/db_metrics_*.json` and prints a CSV for plotting.
- In CI, run the sweep with smaller ranges and assert that average batch ms is under a threshold for default configs.

Findings from recent sweep (summary)
- For the test dataset used in the sweep the `initial_batch` path (chunked, inline CREATE per-chunk) completed the full import in ~200â€“220ms total across capacities. That total was largely insensitive to configured capacity once the dataset fit into one or a small number of chunks.
- The previous `streaming` path frequently produced a single very large transaction and measured ~1.2s total_time for the same dataset.
- A `streaming_chunked` code path was added and fixed to actually split the accumulated entities into chunks; when enabled it matches the low total latency of `initial_batch` by sending multiple smaller transactions instead of one giant one.
- Metrics now include a `total_time_ms` field (sum of per-chunk durations) which should be used to compare end-to-end import time across modes.

Recommended defaults
- Default sensible starting point: `batch_capacity = 500` and `batch_timeout_ms = 100` (these defaults are mirrored in `crates/hyperzoekt/hyperzoekt.toml`).
- Why: the defaults strike a balance between per-transaction latency and throughput for small-to-medium repos; smaller capacities (100) increase the number of transactions but reduce per-batch latency, larger capacities reduce transaction count but offer diminishing throughput returns for this dataset.

Operational notes and gotchas
- The importer switched to using inline JSON CREATE per-chunk when writing transactions (instead of attempting a parameterized `CONTENTS $items`), because some embedded SurrealDB parser versions reject parameterized `CONTENTS` forms. Inline JSON CREATE is more robust across SurrealDB releases.
- To enable the streaming thread to emit chunked transactions use the environment flag `SURREAL_STREAM_CHUNKED=1` (the code still keeps this behind a flag; consider making it the default if you want chunked streaming permanently).
- Use the summarizer script `scripts/metrics_summarize.py` to turn `.data/db_metrics_*.json` into `.data/batch_metrics.csv` and open `.data/batch_sweep_plot.html` (generated by `scripts/generate_plot_html.py`) for an interactive view.

If you want me to (A) make `streaming_chunked` the default and open a PR, (B) re-run the sweep with a different dataset or expanded ranges, or (C) produce a short markdown report including screenshots of the plot, tell me which and I'll continue.

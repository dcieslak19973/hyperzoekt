# SurrealDB Integration Design

This document describes a planned pivot from an in-repo hypergraph representation to using SurrealDB for storing files, entities, and edges produced by the Tree-sitter-based repo indexer.

Goals
- Provide a durable, queryable, and indexable store for entities, files, and typed edges (calls, imports, containment).
- Support both local embedded SurrealDB for development and a remote SurrealDB instance for production.
- Keep the existing `crates/repo-index` code largely unchanged by introducing a small streaming bridge that ingests JSONL and writes to SurrealDB.

Design overview

Modes
- Importer mode (recommended first step): a separate crate `crates/repo-index-importer` reads JSONL (produced by `repo-index`) and upserts into SurrealDB. Supports `--embed` for local embedded SurrealDB or `--surreal-url` to connect remotely.
- Direct streaming mode (future): the indexer itself uses an async writer that upserts records to SurrealDB while indexing. This requires converting the indexer to use an async runtime and adding a `ChannelWriter` bridge (sync -> async) if needed.

Schema (starter)
- `file` table
  - id: autogenerated
  - path: string (indexed)
  - language: string
  - repo: string (optional)
  - indexed_at: datetime

- `entity` table
  - id: autogenerated
  - file -> file:id
  - language: string
  - kind: string (file/class/function/method/other)
  - name: string (indexed)
  - parent -> entity:id (nullable)
  - signature: string
  - start_line: int
  - end_line: int
  - doc: string
  - calls: [string] OR [entity link]
  - rank: float (PageRank or other score)

- `edge` tables (optional)
  - `call`, `import`, `containment` tables with { from -> entity:id, to -> entity:id }
  - Using edge tables allows lightweight graph algorithms and typed queries.

Integration patterns

1) Batch importer (JSONL -> SurrealDB)
- Simple, safe, easy to test and deploy.
- Implementation outline:
  - Read JSONL line-by-line; parse into `Entity` struct.
  - Upsert `file` record for the file path (use a uniqueness constraint / index).
  - Upsert `entity` record with a link to the `file` record.
  - Create edges (call/import/containment) either as links on `entity` records or as separate edge records.
  - Use transactions and batch sizes to improve throughput.
- CLI flags to include:
  - `--surreal-url` (connect to remote)
  - `--embed` (start an embedded SurrealDB instance, file-backed by default)
  - `--batch-size`

2) Streaming (indexer -> SurrealDB)
- Higher complexity, lower latency.
- Convert the CLI to async via tokio.
- Introduce `ChannelWriter` that implements `std::io::Write` and sends JSON lines to an async channel.
- Async DB writer consumes channel, batches upserts, and writes to SurrealDB.
- Implement backpressure and graceful shutdown.

Operational considerations
- Credentials: use env vars `SURREAL_URL`, `SURREAL_NS`, `SURREAL_DB`, `SURREAL_USER`, `SURREAL_PASS`.
- Persistence: default embedded local mode should write to `.data/surrealdb` in the repo root unless overridden.
- Transactions & batching: use transactions for batches of upserts to keep DB consistent and reduce roundtrips.
- Indexes: create indexes on `file.path`, `entity.name`, and any frequently queried fields.
- PageRank: compute offline or in-app and store `entity.rank` in SurrealDB; SurrealDB does not provide PageRank out-of-the-box.

Migration and roll-forward
- Start with the importer and run it over historical JSONL to seed SurrealDB.
- Add read/write tooling and dashboards that query SurrealDB rather than local hypergraph.
- Optionally remove the in-repo hypergraph or keep it for local-only fast queries.

Next steps
1. Implement `crates/repo-index-importer` with `--embed` and `--surreal-url` flags.
2. Add schema creation scripts or queries to initialize indexes.
3. Test importer on a small fixture repo and measure performance.
4. Optionally implement direct streaming via embedding in the indexer once importer is stable.


